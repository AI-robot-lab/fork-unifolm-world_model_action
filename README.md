# UnifoLM-WMA-0: Framework World-Model-Action (WMA) w rodzinie UnifoLM
<p style="font-size: 1.2em;">
    <a href="https://unigen-x.github.io/unifolm-world-model-action.github.io"><strong>Project Page</strong></a> | 
    <a href="https://huggingface.co/collections/unitreerobotics/unifolm-wma-0-68ca23027310c0ca0f34959c"><strong>Models</strong></a> |
    <a href="https://huggingface.co/unitreerobotics/datasets"><strong>Dataset</strong></a> 
  </p>
<div align="center">
  <p align="right">
    <span> ğŸŒEnglish </span> | <a href="README_cn.md"> ğŸ‡¨ğŸ‡³ä¸­æ–‡ </a>
  </p>
</div>
<div align="justify">
    <b>UnifoLM-WMA-0</b> to otwartoÅºrÃ³dÅ‚owa architektura world-modelâ€“action firmy Unitree, dziaÅ‚ajÄ…ca na wielu typach platform robotycznych i przeznaczona do ogÃ³lnego uczenia robotÃ³w. Jej rdzeniem jest world model rozumiejÄ…cy fizyczne interakcje pomiÄ™dzy robotem a Å›rodowiskiem. Model ten zapewnia dwie kluczowe funkcje: (a) <b>Simulation Engine</b> â€“ dziaÅ‚a jako interaktywny symulator i generuje dane syntetyczne do uczenia robota; (b) <b>Policy Enhancement</b> â€“ Å‚Ä…czy siÄ™ z action head i, przewidujÄ…c przyszÅ‚y przebieg interakcji w world model, usprawnia podejmowanie decyzji.
</div>

## ğŸ¯ Cel dydaktyczny i kontekst Unitree G1 EDU-U6
Repozytorium stanowi kompletne Å›rodowisko dydaktyczne do zrozumienia, jak world model i policy head wspÃ³Å‚pracujÄ… przy sterowaniu robotem humanoidalnym. W projekcie laboratorium szczegÃ³lny nacisk kÅ‚adziemy na pracÄ™ z platformÄ… Unitree G1 EDU-U6, dlatego:
- pokazujemy, jak budowaÄ‡ dane wejÅ›ciowe dla world modelu (kamera + stan robota),
- uczymy uruchamiania serwera decyzyjnego oraz klienta robota,
- wyjaÅ›niamy, gdzie modyfikowaÄ‡ konfiguracje, aby dopasowaÄ‡ model do konkretnej wersji sprzÄ™towej.

## ğŸ¦¾ Demonstracje na prawdziwym robocie
| <img src="assets/gifs/real_z1_stackbox.gif" style="border:none;box-shadow:none;margin:0;padding:0;" /> | <img src="assets/gifs/real_dual_stackbox.gif" style="border:none;box-shadow:none;margin:0;padding:0;" /> |
|:---:|:---:|
| <img src="assets/gifs/real_cleanup_pencils.gif" style="border:none;box-shadow:none;margin:0;padding:0;" /> | <img src="assets/gifs/real_g1_pack_camera.gif" style="border:none;box-shadow:none;margin:0;padding:0;" /> |

**Uwaga: w prawym gÃ³rnym oknie widaÄ‡ predykcjÄ™ world modelu dotyczÄ…cÄ… przyszÅ‚ych sekwencji wideo akcji.**

## ğŸ”¥ AktualnoÅ›ci

* 22 wrzeÅ›nia 2025: ğŸš€ UdostÄ™pniliÅ›my kod wdroÅ¼eniowy do eksperymentÃ³w z robotami [Unitree](https://www.unitree.com/).
* 15 wrzeÅ›nia 2025: ğŸš€ UdostÄ™pniliÅ›my kod treningu i inferencji wraz z wagami modelu [**UnifoLM-WMA-0**](https://huggingface.co/collections/unitreerobotics/unifolm-wma-0-68ca23027310c0ca0f34959c).

## ğŸ“‘ Plan otwarcia kodu
- [x] Trening 
- [x] Inferencja
- [x] Checkpointy
- [x] WdroÅ¼enie

## âš™ï¸  Instalacja
PoniÅ¼sze kroki tworzÄ… izolowane Å›rodowisko dla UnifoLM-WMA-0 i instalujÄ… zaleÅ¼noÅ›ci potrzebne do treningu oraz inferencji. DziÄ™ki temu studenci unikajÄ… konfliktÃ³w wersji bibliotek i mogÄ… powtarzalnie uruchamiaÄ‡ pipeline.
```
conda create -n unifolm-wma python==3.10.18
conda activate unifolm-wma

conda install pinocchio=3.2.0 -c conda-forge -y
conda install ffmpeg=7.1.1 -c conda-forge

git clone --recurse-submodules https://github.com/unitreerobotics/unifolm-world-model-action.git

# If you already downloaded the repo:
cd unifolm-world-model-action
git submodule update --init --recursive

pip install -e .

cd external/dlimp
pip install -e .
```
## ğŸ§° Checkpointy modeli
| Model | Opis | Link|
|---------|-------|------|
|$\text{UnifoLM-WMA-0}_{Base}$| Fine-tuning na zbiorze [Open-X](https://robotics-transformer-x.github.io/). | [HuggingFace](https://huggingface.co/unitreerobotics/UnifoLM-WMA-0-Base)|
|$\text{UnifoLM-WMA-0}_{Dual}$| Fine-tuning na piÄ™ciu [zbiorach Unitree opensource](https://huggingface.co/collections/unitreerobotics/g1-dex1-datasets-68bae98bf0a26d617f9983ab) w trybach decision-making i simulation. | [HuggingFace](https://huggingface.co/unitreerobotics/UnifoLM-WMA-0-Dual)|

## ğŸ›¢ï¸ Dataset
W naszych eksperymentach wykorzystujemy nastÄ™pujÄ…ce zbiory opensource:
| ZbiÃ³r danych | Robot | Link |
|---------|-------|------|
|Z1_StackBox| [Unitree Z1](https://www.unitree.com/z1)|[Huggingface](https://huggingface.co/datasets/unitreerobotics/Z1_StackBox_Dataset/tree/v2.1)|
|Z1_DualArm_StackBox|[Unitree Z1](https://www.unitree.com/z1)|[Huggingface](https://huggingface.co/datasets/unitreerobotics/Z1_Dual_Dex1_StackBox_Dataset/tree/v2.1)|
|Z1_DualArm_StackBox_V2|[Unitree Z1](https://www.unitree.com/z1)|[Huggingface](https://huggingface.co/datasets/unitreerobotics/Z1_Dual_Dex1_StackBox_Dataset_V2/tree/v2.1)|
|Z1_DualArm_Cleanup_Pencils|[Unitree Z1](https://www.unitree.com/z1)|[Huggingface](https://huggingface.co/datasets/unitreerobotics/Z1_Dual_Dex1_CleanupPencils_Dataset/tree/v2.1)|
|G1_Pack_Camera|[Unitree G1](https://www.unitree.com/g1)|[Huggingface](https://huggingface.co/datasets/unitreerobotics/G1_Dex1_MountCameraRedGripper_Dataset/tree/v2.1)|

Aby trenowaÄ‡ na wÅ‚asnym zbiorze danych, najpierw przygotuj dane w formacie [Huggingface LeRobot V2.1](https://github.com/huggingface/lerobot). ZaÅ‚Ã³Å¼my, Å¼e struktura katalogÃ³w ÅºrÃ³dÅ‚owych wyglÄ…da tak:
```
source_dir/
    â”œâ”€â”€ dataset1_name
    â”œâ”€â”€ dataset2_name
    â”œâ”€â”€ dataset3_name
    â””â”€â”€ ...
```
NastÄ™pnie skonwertuj dane do wymaganego formatu:
```python
cd prepare_data
python prepare_training_data.py \
    --source_dir /path/to/your/source_dir \
    --target_dir /path/to/save/the/converted/data \
    --dataset_name "dataset1_name" \
    --robot_name "a tag of the robot in the dataset" # e.g, Unitree Z1 Robot Arm or Unitree G1 Robot with Gripper.
```
Struktura wynikowa danych (Uwaga: trening wspiera tylko gÅ‚Ã³wnÄ… kamerÄ™. JeÅ›li dataset ma wiele widokÃ³w, usuÅ„ odpowiednie wartoÅ›ci z kolumny ```data_dir``` w pliku CSV).
```
target_dir/
    â”œâ”€â”€ videos
    â”‚     â”œâ”€â”€dataset1_name
    â”‚     â”‚   â”œâ”€â”€camera_view_dir
    â”‚     â”‚       â”œâ”€â”€ 0.mp4
    â”‚     â”‚       â”œâ”€â”€ 1.mp4
    â”‚     â”‚       â””â”€â”€ ...
    â”‚     â””â”€â”€ ...
    â”œâ”€â”€ transitions
    â”‚    â”œâ”€â”€ dataset1_name
    â”‚        â”œâ”€â”€ meta_data
    â”‚        â”œâ”€â”€ 0.h5
    â”‚        â”œâ”€â”€ 1.h5
    â”‚        â””â”€â”€ ...
    â””â”€â”€  dataset1_name.csv
```
## ğŸš´â€â™‚ï¸ Trening
A. Strategia treningu wyglÄ…da nastÄ™pujÄ…co:
- **Step 1**: Fine-tuning modelu generacji wideo jako world model na zbiorze [Open-X](https://robotics-transformer-x.github.io/); *Cel:* nauczenie modelu fizyki i dynamiki scen.
- **Step 2**: Post-trening $\text{UnifoLM-WMA}$ w trybie decision-making na docelowym zbiorze zadaÅ„; *Cel:* dopasowanie polityki do konkretnego zadania robota.
  <div align="left">
      <img src="assets/pngs/dm_mode.png" width="600">
  </div>
- **Step 3**: Post-trening $\text{UnifoLM-WMA}$ w trybie simulation na docelowym zbiorze zadaÅ„. *Cel:* poprawa jakoÅ›ci symulacji i przewidywania przyszÅ‚ych interakcji.
  <div align="left">
      <img src="assets/pngs/sim_mode.png" width="600">
  </div>
**Uwaga**: JeÅ›li potrzebujesz $\text{UnifoLM-WMA}$ tylko w jednym trybie, moÅ¼esz pominÄ…Ä‡ odpowiedni krok.

B. Aby przeprowadziÄ‡ trening na jednym lub wielu zbiorach danych, wykonaj:
- **Step 1**: Maksymalne DoF ustawiono na 16; jeÅ›li masz wiÄ™cej DoF, zaktualizuj ```agent_state_dim``` i ```agent_action_dim``` w [configs/train/config.yaml](https://github.com/unitreerobotics/unifolm-wma/blob/working/configs/train/config.yaml). *Cel:* zgodnoÅ›Ä‡ wymiarÃ³w stanu/akcji z realnym robotem.
- **Step 2**: Ustaw ksztaÅ‚ty wejÅ›Ä‡ dla kaÅ¼dej modalnoÅ›ci w [configs/train/meta.json](https://github.com/unitreerobotics/unitree-world-model/blob/main/configs/train/meta.json). *Cel:* poprawne mapowanie obrazÃ³w i stanu do modelu.
- **Step 3**: Skonfiguruj parametry treningu w [configs/train/config.yaml](https://github.com/unitreerobotics/unitree-world-model/blob/main/configs/train/config.yaml). Dla ```pretrained_checkpoint``` zalecamy " $\text{UnifoLM-WMA-0}_{Base}$ " wytrenowany na [Open-X](https://robotics-transformer-x.github.io/). *Cel:* start z sprawdzonych wag.
  ```yaml
  model:
      pretrained_checkpoint: /path/to/pretrained/checkpoint;
      ...
      decision_making_only: True # Train the world model only in decision-making mode. If False, jointly train it in both decision-making and simulation modes.
      ...
  data:
      ...
      train:
          ...
          data_dir: /path/to/training/dataset/directory
      dataset_and_weights: # list the name of each dataset below and make sure the summation of weights is 1.0
          dataset1_name: 0.2
          dataset2_name: 0.2
          dataset3_name: 0.2
      dataset4_name: 0.2
      dataset5_name: 0.2
  ```
- **Step 4**: Ustaw ```experiment_name``` oraz ```save_root``` w [scripts/train.sh](https://github.com/unitreerobotics/unitree-world-model/blob/main/scripts/train.sh). *Cel:* uporzÄ…dkowany zapis wynikÃ³w.
- **Step 5**: Uruchom trening:
```
bash scripts/train.sh
```
## ğŸŒ Inferencja w trybie Interactive Simulation
Aby uruchomiÄ‡ world model w trybie interaktywnej symulacji, wykonaj:
- **Step 1**: (PomiÅ„, jeÅ›li uÅ¼ywasz dostarczonych przykÅ‚adÃ³w) Przygotuj wÅ‚asny prompt zgodnie z formatem z [examples/world_model_interaction_prompts](https://github.com/unitreerobotics/unitree-world-model/tree/main/examples/world_model_interaction_prompts). *Cel:* zapewnienie spÃ³jnego wejÅ›cia obraz + instrukcja.
  ```
  world_model_interaction_prompts/
    â”œâ”€â”€ images
    â”‚    â”œâ”€â”€ dataset1_name
    â”‚    â”‚       â”œâ”€â”€ 0.png     # Image prompt
    â”‚    â”‚       â””â”€â”€ ...
    â”‚    â””â”€â”€ ...
    â”œâ”€â”€ transitions
    â”‚    â”œâ”€â”€ dataset1_name
    â”‚    â”‚       â”œâ”€â”€ meta_data # Used for normalization
    â”‚    â”‚       â”œâ”€â”€ 0.h       # Robot state and action data; in interaction mode,
    â”‚    â”‚       â”‚             # only used to retrieve the robot state corresponding 
    â”‚    â”‚       â”‚             # to the image prompt
    â”‚    â”‚       â””â”€â”€ ...
    â”‚    â””â”€â”€ ...
    â”œâ”€â”€  dataset1_name.csv     # File for loading image prompts, text instruction and corresponding robot states
    â””â”€â”€ ...
  ```
- **Step 2**: WskaÅ¼ poprawne Å›cieÅ¼ki dla ```pretrained_checkpoint``` (np. $\text{UnifoLM-WMA-0}_{Dual}$) i ```data_dir``` w [configs/inference/world_model_interaction.yaml](https://github.com/unitreerobotics/unitree-world-model/blob/main/configs/inference/world_model_interaction.yaml). *Cel:* model musi wiedzieÄ‡, skÄ…d pobraÄ‡ wagi i dane.
- **Step 3**: Ustaw ```checkpoint```, ```res_dir``` i ```prompt_dir``` w [scripts/run_world_model_interaction.sh](https://github.com/unitreerobotics/unitree-world-model/blob/main/scripts/run_world_model_interaction.sh) oraz wpisz nazwy datasetÃ³w w ```datasets=(...)```. NastÄ™pnie uruchom:
    ```
    bash scripts/run_world_model_interaction.sh
    ```

## ğŸ§  Inferencja i wdroÅ¼enie w trybie Decision-Making

W tym wariancie inferencja dziaÅ‚a na serwerze, a klient robota zbiera obserwacje z robota rzeczywistego i wysyÅ‚a je do serwera po akcje. Ten podziaÅ‚ uÅ‚atwia uruchomienie ciÄ™Å¼kich obliczeÅ„ na GPU oraz utrzymuje szybki loop sterowania na robocie.

### Konfiguracja serwera:
- **Step-1**: Ustaw ```ckpt```, ```res_dir```, ```datasets``` w [scripts/run_real_eval_server.sh](https://github.com/unitreerobotics/unifolm-world-model-action/blob/main/scripts/run_real_eval_server.sh). *Cel:* wskazanie wag modelu i katalogu wynikÃ³w.
- **Step-2**: Skonfiguruj ```data_dir``` oraz ```dataset_and_weights``` w [config/inference/world_model_decision_making.yaml](https://github.com/unitreerobotics/unifolm-world-model-action/blob/f12b4782652ca00452941d851b17446e4ee7124a/configs/inference/world_model_decision_making.yaml#L225). *Cel:* poprawne wczytanie danych wejÅ›ciowych.
- **Step-3**: Uruchom serwer:
```
conda activate unifolm-wma
cd unifolm-world-model-action
bash scripts/run_real_eval_server.sh
```

### Konfiguracja klienta
- **Step-1**: Wykonaj instrukcje z [unitree_deploy/README.md](https://github.com/unitreerobotics/unifolm-world-model-action/blob/main/unitree_deploy/README.md), aby utworzyÄ‡ Å›rodowisko ```unitree_deploy```, zainstalowaÄ‡ pakiety i uruchomiÄ‡ kontrolery na robocie.
- **Step-2**: OtwÃ³rz nowy terminal i zestaw tunel SSH z klienta do serwera:
```
ssh user_name@remote_server_IP -CNg -L 8000:127.0.0.1:8000
```
- **Step-3**: Uruchom ```unitree_deploy/robot_client.py```, aby rozpoczÄ…Ä‡ inferencjÄ™:
```
cd unitree_deploy
python scripts/robot_client.py --robot_type "g1_dex1" --action_horizon 16 --exe_steps 16 --observation_horizon 2 --language_instruction "pack black camera into box" --output_dir ./results --control_freq 15
```

## ğŸ“ Zastosowanie w projekcie Unitree G1 EDU-U6
PoniÅ¼sza sekcja podsumowuje, jak studenci mogÄ… wykorzystaÄ‡ repozytorium w projekcie z robotem humanoidalnym Unitree G1 EDU-U6:
- **Cel projektu**: nauczyÄ‡ siÄ™, jak world model wspiera sterowanie robotem, przewidywanie przyszÅ‚ych stanÃ³w oraz planowanie ruchu.
- **Zakres prac**: przygotowanie danych z kamery i stanu robota, uruchomienie serwera decision-making, wywoÅ‚anie klienta na robocie oraz analiza wygenerowanych akcji.
- **Praktyczny scenariusz**:
  1) Zbierz krÃ³tkie demonstracje z G1 EDU-U6 (kamera + qpos) i przekonwertuj je do formatu LeRobot.
  2) Skonfiguruj training na bazie $\text{UnifoLM-WMA-0}_{Base}$, aby dostroiÄ‡ model do zadania (np. pakowanie obiektu).
  3) Uruchom serwer inferencji na stacji z GPU i klienta na robocie, przekazujÄ…c instrukcjÄ™ jÄ™zykowÄ… oraz parametry horyzontu akcji.
  4) Przeanalizuj zapisane wideo i logi, aby oceniÄ‡ stabilnoÅ›Ä‡ i jakoÅ›Ä‡ dziaÅ‚ania.

## ğŸ“ Architektura kodu
PoniÅ¼ej znajduje siÄ™ uproszczony przeglÄ…d struktury i gÅ‚Ã³wnych komponentÃ³w projektu:
```
unitree-world-model/
    â”œâ”€â”€ assets                      # Media (GIF, obrazy, wideo demonstracyjne)
    â”œâ”€â”€ configs                     # Konfiguracje treningu i inferencji
    â”‚    â”œâ”€â”€ inference
    â”‚    â””â”€â”€  train
    â”œâ”€â”€ examples                    # PrzykÅ‚adowe wejÅ›cia i prompty do inferencji
    â”œâ”€â”€ external                    # Pakiety zewnÄ™trzne
    â”œâ”€â”€ prepare_data                # Skrypty preprocessingu i konwersji datasetÃ³w
    â”œâ”€â”€ scripts                     # Skrypty treningu, ewaluacji i wdroÅ¼enia
    â”œâ”€â”€ src
    â”‚    â”œâ”€â”€unitree_worldmodel      # GÅ‚Ã³wny pakiet Pythona dla Unitree world model
    â”‚    â”‚      â”œâ”€â”€ data            # Åadowanie datasetÃ³w, transformacje, dataloadery
    â”‚    â”‚      â”œâ”€â”€ models          # Architektury modeli i backbone
    â”‚    â”‚      â”œâ”€â”€ modules         # ModuÅ‚y i komponenty modelu
    â”‚    â”‚      â””â”€â”€  utils          # NarzÄ™dzia pomocnicze
    â””â”€â”€ unitree_deploy              # Kod wdroÅ¼eniowy
```

## ğŸ™ PodziÄ™kowania
DuÅ¼a czÄ™Å›Ä‡ kodu pochodzi z [DynamiCrafter](https://github.com/Doubiiu/DynamiCrafter), [Diffusion Policy](https://github.com/real-stanford/diffusion_policy), [ACT](https://github.com/MarkFzp/act-plus-plus) oraz [HPT](https://github.com/liruiw/HPT).

## ğŸ“ Cytowanie
```
@misc{unifolm-wma-0,
  author       = {Unitree},
  title        = {UnifoLM-WMA-0: A World-Model-Action (WMA) Framework under UnifoLM Family},
  year         = {2025},
}
```
